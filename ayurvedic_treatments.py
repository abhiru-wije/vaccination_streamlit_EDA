# -*- coding: utf-8 -*-
"""Ayurvedic_treatments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XCscx7juigdKcVTuhRjdGP3CSeqbqoUR
"""

import numpy as np  #numerical calculations
import pandas as pd #data frames
import matplotlib.pyplot as plt  #visualization
import seaborn as sns #visualization

#loading datasets from 2020 to 2024
df1 = pd.read_csv('Final_Dataset_2020.csv')
df2 = pd.read_csv('Final_Dataset_2021.csv')
df3 = pd.read_csv('Final_Dataset_2022.csv')
df4 = pd.read_csv('Final_Dataset_2023.csv')
df5 = pd.read_csv('Final_Dataset_2024.csv')

# Merging five datasets
combined_df = pd.concat([df1, df2, df3, df4], axis=0)

# Write the combined DataFrame to a new CSV file
combined_df.to_csv('data.csv', index=False)

df_final= pd.read_csv('data.csv')

# loading first few rows of final dataset
df_final.head()

#Loading last five rows of the final dataset
df_final.tail()

#shape of the final dataset
print(f'Shape of the dataset: {df_final.shape}')

#viewing data types of the each columns
print(df_final.dtypes)

# Summary statistics for numerical columns
summary_statistics = df_final.describe()
print(summary_statistics)

# List of attendance columns
attendance_columns = ['1st Att', '2nd Att', '3rd Att', '4th Att', '5th Att', '6th Att', '7th Att', '8th Att', '9th Att', '10th Att', '11th Att']

# Calculate the number of non-null visits for each patient
df_final['Number of Visits'] = df_final[attendance_columns].notnull().sum(axis=1)

# Save the updated dataset
df_final.to_csv('data_f.csv', index=False)

print("Column for number of visits per patient added and data saved.")

numerical_columns = ['Age', 'Weight', 'Height', 'Number of Visits']  # Adjust this list based on your actual column names

# Calculate the correlation matrix
correlation_matrix = df_final[numerical_columns].corr()

# Plot the correlation matrix using seaborn
plt.figure(figsize=(8, 6))  # Adjust the figure size as necessary
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", cbar_kws={'label': 'Correlation Coefficient'})
plt.title('Correlation Matrix for Numerical Variables')
plt.show()

df_final = pd.read_csv("data_f.csv")

print(df_final.dtypes)

#missing values analysis
missing_values = df_final.isnull().mean() * 100
plt.figure(figsize=(12, 8))
missing_values.sort_values(ascending=False).plot(kind='bar')
plt.title('Percentage of Missing Values by Column')
plt.xlabel('Columns')
plt.ylabel('Percentage of Missing Values')
plt.show()

#dropping the columns which are more than 15% missing values
columns_to_drop = missing_values[missing_values > 15].index
df_final.drop(columns=columns_to_drop, inplace=True)

# Drop rows where 'Vaccination' or 'City' columns have missing values
df_final = df_final.dropna(subset=['Vaccination', 'City','Discription','date','Medication','1st Att'])

#missing values analysis after missing value treatment
missing_percentage = df_final.isnull().sum() * 100 / len(df_final)
print("Missing Value Percentages by Column:")
print(missing_percentage)

#Handling duplicate values
print("Number of duplicate records before removal:", df_final.duplicated().sum())

#EDA

#Age group Analysis
bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
labels = ['1-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']

df_final['Age_Group'] = pd.cut(df_final['Age'], bins=bins, labels=labels, right=False)

# Check the distribution of age groups
age_group_distribution = df_final['Age_Group'].value_counts().sort_index()
print(age_group_distribution)

# Set the style of seaborn
sns.set(style="whitegrid")

# Plot the distribution of age groups
plt.figure(figsize=(10, 6))
sns.countplot(x='Age_Group', data=df_final, palette="coolwarm")
plt.title('Distribution of Age Groups')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Gender Analysis
sns.set(style="whitegrid")
# Create the bar chart
plt.figure(figsize=(8, 5))
sns.countplot(x='Gender', data=df_final, palette='coolwarm')
plt.title('Gender Distribution')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

#making seperate column for BMI
df_final['Height_m'] = df_final['Height'] * 0.3048
df_final['BMI'] = df_final['Weight'] / (df_final['Height_m'] ** 2)

#visualizing BMI distribution of the patients
plt.figure(figsize=(10, 6))
plt.hist(df_final['BMI'], bins=30, color='skyblue', edgecolor='black')
plt.title('BMI Distribution')
plt.xlabel('BMI')
plt.ylabel('Frequency')
plt.show()

#vaccination status Analysis
sns.set(style="whitegrid")
plt.figure(figsize=(8, 5))
sns.countplot(x='Vaccination', data=df_final, palette='coolwarm')
plt.title('Vaccination Status')
plt.xlabel('Vaccination')
plt.ylabel('Count')
plt.show()

# Ensure the 'date' column is in datetime format
df_final['date'] = pd.to_datetime(df_final['date'], errors='coerce')
df_final['Year'] = df_final['date'].dt.year

# Categorize as Pre-COVID and Post-COVID
df_final['COVID_Period'] = df_final['Year'].apply(
    lambda x: 'Pre-COVID' if x in [2020, 2021]
              else 'Post-COVID' if x in [2022, 2023, 2024]
              else 'Unknown'
)

# Filter out 'Unknown' categories
filtered_df = df_final[df_final['COVID_Period'] != 'Unknown']

plt.figure(figsize=(8, 5))
sns.countplot(x='COVID_Period', data=filtered_df, palette='coolwarm')
plt.title('Distribution of Pre-COVID and Post-COVID Records')
plt.xlabel('COVID Period')
plt.ylabel('Count')
plt.show()

import plotly.express as px
fig = px.histogram(df_final, x='Number of Visits', nbins=50, title='Distribution of Number of Visits')
fig.update_layout(xaxis_title='Number of Visits', yaxis_title='Count of Patients')
fig.show()

pip install wordcloud

from wordcloud import WordCloud

# Concatenate all the cleaned symptoms into a single string
text = " ".join(symptom for symptom in df_final['cleaned_symptoms'].dropna())

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the generated image:
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Word Cloud for Cleaned Symptoms')
plt.show()

# Descriptive statistics for number of visits
visits_pre_covid = filtered_df[filtered_df['COVID_Period'] == 'Pre-COVID']['Number of Visits']
visits_post_covid = filtered_df[filtered_df['COVID_Period'] == 'Post-COVID']['Number of Visits']
print(visits_pre_covid.describe(), visits_post_covid.describe())

# Set up visualization
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 12))

# Comparing BMI Distribution of pre covid qnd post covid
sns.histplot(df_final[df_final['COVID_Period'] == 'Pre-COVID']['BMI'], bins=30, kde=True, color='blue', ax=axes[0, 0])
axes[0, 0].set_title('Pre-COVID BMI Distribution')
sns.histplot(df_final[df_final['COVID_Period'] == 'Post-COVID']['BMI'], bins=30, kde=True, color='green', ax=axes[0, 1])
axes[0, 1].set_title('Post-COVID BMI Distribution')

#  Comparing Vaccination Status of pre-covid and post-covid
sns.countplot(x='Vaccination', data=df_final[df_final['COVID_Period'] == 'Pre-COVID'], palette='coolwarm', ax=axes[1, 0])
axes[1, 0].set_title('Pre-COVID Vaccination Status')
sns.countplot(x='Vaccination', data=df_final[df_final['COVID_Period'] == 'Post-COVID'], palette='coolwarm', ax=axes[1, 1])
axes[1, 1].set_title('Post-COVID Vaccination Status')

plt.tight_layout()
plt.show()

from wordcloud import WordCloud

# Concatenate symptoms into a single string for each period
text_pre_covid = " ".join(symptom for symptom in df_final[df_final['COVID_Period'] == 'Pre-COVID']['cleaned_symptoms'].dropna())
text_post_covid = " ".join(symptom for symptom in df_final[df_final['COVID_Period'] == 'Post-COVID']['cleaned_symptoms'].dropna())

# Generate word clouds
wordcloud_pre_covid = WordCloud(width=800, height=400, background_color='white').generate(text_pre_covid)
wordcloud_post_covid = WordCloud(width=800, height=400, background_color='white').generate(text_post_covid)

# Display the word clouds
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))
axes[0].imshow(wordcloud_pre_covid, interpolation='bilinear')
axes[0].axis("off")
axes[0].set_title('Pre-COVID Symptoms Word Cloud')
axes[1].imshow(wordcloud_post_covid, interpolation='bilinear')
axes[1].axis("off")
axes[1].set_title('Post-COVID Symptoms Word Cloud')

plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline

df_final['High_Frequency_Visit'] = (df_final['Number of Visits'] > 3).astype(int)

# Count the occurrences of high and low frequency visits
visit_counts = df_final['High_Frequency_Visit'].value_counts()

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(visit_counts, labels=['Not High Frequency', 'High Frequency'], autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightgreen'])
plt.title('Distribution of High Frequency Visits')
plt.show()

features = df_final[['Age', 'BMI', 'Gender', 'Vaccination']]

# Convert categorical data to dummy variables
features_final = pd.get_dummies(df_final[['Age', 'BMI', 'Gender', 'Vaccination']], drop_first=True)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features_final, df_final['High_Frequency_Visit'], test_size=0.2, random_state=42)

# Initialize and train the classifier
classifier = RandomForestClassifier(random_state=42)
classifier.fit(X_train, y_train)

# Predict and evaluate the model
predictions = classifier.predict(X_test)
print(classification_report(y_test, predictions))

# Initialize the models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    #"SVM": SVC(kernel='linear', probability=True),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42)
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
    results[name] = {'Accuracy': accuracy, 'ROC AUC': roc_auc}

# Print results
for model in results:
    print(f"{model}:\n Accuracy: {results[model]['Accuracy']:.2f}, ROC AUC: {results[model]['ROC AUC']:.2f}\n")

model_names = ['Logistic Regression', 'Random Forest', 'SVM', 'Gradient Boosting']
accuracies = [0.71, 0.65, 0.71, 0.70]
roc_aucs = [0.62, 0.53, 0.61, 0.61]

x = np.arange(len(model_names))
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy')
rects2 = ax.bar(x + width/2, roc_aucs, width, label='ROC AUC')

ax.set_xlabel('Models')
ax.set_title('Comparison of Machine Learning Models')
ax.set_xticks(x)
ax.set_xticklabels(model_names)
ax.legend()

def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)
fig.tight_layout()

plt.show()

from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight('balanced', classes=[0, 1], y=y_train)
class_weight_dict = {0: class_weights[0], 1: class_weights[1]}

# Updated model initialization with class_weight
model = LogisticRegression(max_iter=1000, class_weight=class_weight_dict)

# Revised grid for solvers and penalties
param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs'],  # Both support 'l2'
    'penalty': ['l2']
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best cross-validation score: {:.2f}".format(grid_search.best_score_))

# Evaluate on the test set again
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
print(classification_report(y_test, y_pred))

# Define the SMOTE method
smote = SMOTE(random_state=42)

# Resample the training set
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Define a pipeline to scale features and train a classifier
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Feature scaling
    ('classifier', RandomForestClassifier(random_state=42))
])

# Train the model using the resampled data
pipeline.fit(X_train_smote, y_train_smote)

# Predict on the test set and evaluate
y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))

# Define the model
model = RandomForestClassifier(random_state=42)

# Create the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt']
}

# Setup the grid search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)
grid_search.fit(X_train, y_train)

# Print the best parameters and best score
print("Best parameters:", grid_search.best_params_)
print("Best cross-validation score: {:.2f}".format(grid_search.best_score_))

# Evaluate on the test set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
print(classification_report(y_test, y_pred))